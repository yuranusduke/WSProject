subject#subsubject#subsubsubject#title#authors#abstract#comment
computer science#computing research repository#computer vision and pattern recognition#READ: Large-Scale Neural Scene Rendering for Autonomous Driving#Zhuopeng Li, Lu Li, Zeyu Ma, Ping Zhang, Junbo Chen, Jianke Zhu#"Synthesizing free-view photo-realistic images is an important task in
multimedia. With the development of advanced driver assistance systems~(ADAS)
and their applications in autonomous vehicles, experimenting with different
scenarios becomes a challenge. Although the photo-realistic street scenes can
be synthesized by image-to-image translation methods, which cannot produce
coherent scenes due to the lack of 3D information. In this paper, a large-scale
neural rendering method is proposed to synthesize the autonomous driving
scene~(READ), which makes it possible to synthesize large-scale driving
scenarios on a PC through a variety of sampling schemes. In order to represent
driving scenarios, we propose an {\omega} rendering network to learn neural
descriptors from sparse point clouds. Our model can not only synthesize
realistic driving scenes but also stitch and edit driving scenes. Experiments
show that our model performs well in large-scale driving scenarios."#
computer science#computing research repository#computer vision and pattern recognition#NTIRE 2022 Challenge on Efficient Super-Resolution: Methods and Results#Yawei Li, Kai Zhang, Radu Timofte, Luc Van Gool, Fangyuan Kong, Mingxi Li, Songwei Liu, Zongcai Du, Ding Liu, Chenhui Zhou, Jingyi Chen, Qingrui Han, Zheyuan Li, Yingqi Liu, Xiangyu Chen, Haoming Cai, Yu Qiao, Chao Dong, Long Sun, Jinshan Pan, Yi Zhu, Zhikai Zong, Xiaoxiao Liu, Zheng Hui, Tao Yang, Peiran Ren, Xuansong Xie, Xian-Sheng Hua, Yanbo Wang, Xiaozhong Ji, Chuming Lin, Donghao Luo, Ying Tai, Chengjie Wang, Zhizhong Zhang, Yuan Xie, Shen Cheng, Ziwei Luo, Lei Yu, Zhihong Wen, Qi Wu1, Youwei Li, Haoqiang Fan, Jian Sun, Shuaicheng Liu, Yuanfei Huang, Meiguang Jin, Hua Huang, Jing Liu, Xinjian Zhang, Yan Wang, Lingshun Long, Gen Li, Yuanfan Zhang, Zuowei Cao, Lei Sun, Panaetov Alexander, Yucong Wang, Minjie Cai, Li Wang, Lu Tian, Zheyuan Wang, Hongbing Ma, Jie Liu, Chao Chen, Yidong Cai, Jie Tang, Gangshan Wu, Weiran Wang, Shirui Huang, Honglei Lu, Huan Liu, Keyan Wang, Jun Chen, Shi Chen, Yuchun Miao, Zimo Huang, Lefei Zhang, Mustafa Ayazoğlu, Wei Xiong, Chengyi Xiong, Fei Wang, Hao Li, Ruimian Wen, Zhijing Yang, Wenbin Zou, Weixin Zheng, Tian Ye, Yuncheng Zhang, Xiangzhen Kong, Aditya Arora, Syed Waqas Zamir, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Dandan Gaoand Dengwen Zhouand Qian Ning, Jingzhu Tang, Han Huang, Yufei Wang, Zhangheng Peng, Haobo Li, Wenxue Guan, Shenghua Gong, Xin Li, Jun Liu, Wanjun Wang, Dengwen Zhou, Kun Zeng, Hanjiang Lin, Xinyu Chen, Jinsheng Fang, et al. (11 additional authors not shown)#"This paper reviews the NTIRE 2022 challenge on efficient single image
super-resolution with focus on the proposed solutions and results. The task of
the challenge was to super-resolve an input image with a magnification factor
of $\times$4 based on pairs of low and corresponding high resolution images.
The aim was to design a network for single image super-resolution that achieved
improvement of efficiency measured according to several metrics including
runtime, parameters, FLOPs, activations, and memory consumption while at least
maintaining the PSNR of 29.00dB on DIV2K validation set. IMDN is set as the
baseline for efficiency measurement. The challenge had 3 tracks including the
main track (runtime), sub-track one (model complexity), and sub-track two
(overall performance). In the main track, the practical runtime performance of
the submissions was evaluated. The rank of the teams were determined directly
by the absolute value of the average runtime on the validation set and test
set. In sub-track one, the number of parameters and FLOPs were considered. And
the individual rankings of the two metrics were summed up to determine a final
ranking in this track. In sub-track two, all of the five metrics mentioned in
the description of the challenge including runtime, parameter count, FLOPs,
activations, and memory consumption were considered. Similar to sub-track one,
the rankings of five metrics were summed up to determine a final ranking. The
challenge had 303 registered participants, and 43 teams made valid submissions.
They gauge the state-of-the-art in efficient single image super-resolution."#Validation code of the baseline model is available at . Validation of all submitted models is available at
computer science#computing research repository#computer vision and pattern recognition#TDT: Teaching Detectors to Track without Fully Annotated Videos#Shuzhi Yu, Guanhang Wu, Chunhui Gu, Mohammed E. Fathy#"Recently, one-stage trackers that use a joint model to predict both
detections and appearance embeddings in one forward pass received much
attention and achieved state-of-the-art results on the Multi-Object Tracking
(MOT) benchmarks. However, their success depends on the availability of videos
that are fully annotated with tracking data, which is expensive and hard to
obtain. This can limit the model generalization. In comparison, the two-stage
approach, which performs detection and embedding separately, is slower but
easier to train as their data are easier to annotate. We propose to combine the
best of the two worlds through a data distillation approach. Specifically, we
use a teacher embedder, trained on Re-ID datasets, to generate pseudo
appearance embedding labels for the detection datasets. Then, we use the
augmented dataset to train a detector that is also capable of regressing these
pseudo-embeddings in a fully-convolutional fashion. Our proposed one-stage
solution matches the two-stage counterpart in quality but is 3 times faster.
Even though the teacher embedder has not seen any tracking data during
training, our proposed tracker achieves competitive performance with some
popular trackers (e.g. JDE) trained with fully labeled tracking data."#Workshop on Learning with Limited Labelled Data for Image and Video Understanding (L3D-IVU), CVPR2022 Workshop
computer science#computing research repository#computer vision and pattern recognition#NMR: Neural Manifold Representation for Autonomous Driving#Unnikrishnan R. Nair, Sarthak Sharma, Midhun S. Menon, Srikanth Vidapanakal#"Autonomous driving requires efficient reasoning about the Spatio-temporal
nature of the semantics of the scene. Recent approaches have successfully
amalgamated the traditional modular architecture of an autonomous driving stack
comprising perception, prediction, and planning in an end-to-end trainable
system. Such a system calls for a shared latent space embedding with
interpretable intermediate trainable projected representation. One such
successfully deployed representation is the Bird's-Eye View(BEV) representation
of the scene in ego-frame. However, a fundamental assumption for an undistorted
BEV is the local coplanarity of the world around the ego-vehicle. This
assumption is highly restrictive, as roads, in general, do have gradients. The
resulting distortions make path planning inefficient and incorrect. To overcome
this limitation, we propose Neural Manifold Representation (NMR), a
representation for the task of autonomous driving that learns to infer
semantics and predict way-points on a manifold over a finite horizon, centered
on the ego-vehicle. We do this using an iterative attention mechanism applied
on a latent high dimensional embedding of surround monocular images and partial
ego-vehicle state. This representation helps generate motion and behavior plans
consistent with and cognizant of the surface geometry. We propose a sampling
algorithm based on edge-adaptive coverage loss of BEV occupancy grid and
associated guidance flow field to generate the surface manifold while incurring
minimal computational overhead. We aim to test the efficacy of our approach on
CARLA and SYNTHIA-SF."#
computer science#computing research repository#computer vision and pattern recognition#RepSR: Training Efficient VGG-style Super-Resolution Networks with Structural Re-Parameterization and Batch Normalization#Xintao Wang, Chao Dong, Ying Shan#"This paper explores training efficient VGG-style super-resolution (SR)
networks with the structural re-parameterization technique. The general
pipeline of re-parameterization is to train networks with multi-branch topology
first, and then merge them into standard 3x3 convolutions for efficient
inference. In this work, we revisit those primary designs and investigate
essential components for re-parameterizing SR networks. First of all, we find
that batch normalization (BN) is important to bring training non-linearity and
improve the final performance. However, BN is typically ignored in SR, as it
usually degrades the performance and introduces unpleasant artifacts. We
carefully analyze the cause of BN issue and then propose a straightforward yet
effective solution. In particular, we first train SR networks with mini-batch
statistics as usual, and then switch to using population statistics at the
later training period. While we have successfully re-introduced BN into SR, we
further design a new re-parameterizable block tailored for SR, namely RepSR. It
consists of a clean residual path and two expand-and-squeeze convolution paths
with the modified BN. Extensive experiments demonstrate that our simple RepSR
is capable of achieving superior performance to previous SR re-parameterization
methods among different model sizes. In addition, our RepSR can achieve a
better trade-off between performance and actual running time (throughput) than
previous SR methods. Codes will be available at
."#Technical Report. Codes will be available at
computer science#computing research repository#computer vision and pattern recognition#Video-ReTime: Learning Temporally Varying Speediness for Time Remapping#Simon Jenni, Markus Woodson, Fabian Caba Heilbron#"We propose a method for generating a temporally remapped video that matches
the desired target duration while maximally preserving natural video dynamics.
Our approach trains a neural network through self-supervision to recognize and
accurately localize temporally varying changes in the video playback speed. To
re-time videos, we 1. use the model to infer the slowness of individual video
frames, and 2. optimize the temporal frame sub-sampling to be consistent with
the model's slowness predictions. We demonstrate that this model can detect
playback speed variations more accurately while also being orders of magnitude
more efficient than prior approaches. Furthermore, we propose an optimization
for video re-timing that enables precise control over the target duration and
performs more robustly on longer videos than prior methods. We evaluate the
model quantitatively on artificially speed-up videos, through transfer to
action recognition, and qualitatively through user studies."#Accepted at the AI for Content Creation (AICC) workshop at CVPR 2022
computer science#computing research repository#computer vision and pattern recognition#Revisiting Random Channel Pruning for Neural Network Compression#Yawei Li, Kamil Adamczewski, Wen Li, Shuhang Gu, Radu Timofte, Luc Van Gool#"Channel (or 3D filter) pruning serves as an effective way to accelerate the
inference of neural networks. There has been a flurry of algorithms that try to
solve this practical problem, each being claimed effective in some ways. Yet, a
benchmark to compare those algorithms directly is lacking, mainly due to the
complexity of the algorithms and some custom settings such as the particular
network configuration or training procedure. A fair benchmark is important for
the further development of channel pruning.
Meanwhile, recent investigations reveal that the channel configurations
discovered by pruning algorithms are at least as important as the pre-trained
weights. This gives channel pruning a new role, namely searching the optimal
channel configuration. In this paper, we try to determine the channel
configuration of the pruned models by random search. The proposed approach
provides a new way to compare different methods, namely how well they behave
compared with random pruning. We show that this simple strategy works quite
well compared with other channel pruning methods. We also show that under this
setting, there are surprisingly no clear winners among different channel
importance evaluation methods, which then may tilt the research efforts into
advanced channel configuration searching methods."#Accepted to CVPR2022. Code will be released at \url{}
computer science#computing research repository#computer vision and pattern recognition#An Empirical Study Of Self-supervised Learning Approaches For Object Detection With Transformers#Gokul Karthik Kumar, Sahal Shaji Mullappilly, Abhishek Singh Gehlot#"Self-supervised learning (SSL) methods such as masked language modeling have
shown massive performance gains by pretraining transformer models for a variety
of natural language processing tasks. The follow-up research adapted similar
methods like masked image modeling in vision transformer and demonstrated
improvements in the image classification task. Such simple self-supervised
methods are not exhaustively studied for object detection transformers (DETR,
Deformable DETR) as their transformer encoder modules take input in the
convolutional neural network (CNN) extracted feature space rather than the
image space as in general vision transformers. However, the CNN feature maps
still maintain the spatial relationship and we utilize this property to design
self-supervised learning approaches to train the encoder of object detection
transformers in pretraining and multi-task learning settings. We explore common
self-supervised methods based on image reconstruction, masked image modeling
and jigsaw. Preliminary experiments in the iSAID dataset demonstrate faster
convergence of DETR in the initial epochs in both pretraining and multi-task
learning settings; nonetheless, similar improvement is not observed in the case
of multi-task learning with Deformable DETR. The code for our experiments with
DETR and Deformable DETR are available at 
and  respectively."#"Final Project for the course ""Visual Object Detection And Recognition"" (CV703) at MBZUAI"
computer science#computing research repository#computer vision and pattern recognition#Face Detection on Mobile: Five Implementations and Analysis#Kostiantyn Khabarlak#"In many practical cases face detection on smartphones or other highly
portable devices is a necessity. Applications include mobile face access
control systems, driver status tracking, emotion recognition, etc. Mobile
devices have limited processing power and should have long-enough battery life
even with face detection application running. Thus, striking the right balance
between algorithm quality and complexity is crucial. In this work we adapt 5
algorithms to mobile. These algorithms are based on handcrafted or
neural-network-based features and include: Viola-Jones (Haar cascade), LBP,
HOG, MTCNN, BlazeFace. We analyze inference time of these algorithms on
different devices with different input image resolutions. We provide guidance,
which algorithms are the best fit for mobile face access control systems and
potentially other mobile applications. Interestingly, we note that cascaded
algorithms perform faster on scenes without faces, while BlazeFace is slower on
empty scenes. Exploiting this behavior might be useful in practice."#
computer science#computing research repository#computer vision and pattern recognition#Review on Panoramic Imaging and Its Applications in Scene Understanding#Shaohua Gao, Kailun Yang, Hao Shi, Kaiwei Wang, Jian Bai#"With the rapid development of high-speed communication and artificial
intelligence technologies, human perception of real-world scenes is no longer
limited to the use of small Field of View (FoV) and low-dimensional scene
detection devices. Panoramic imaging emerges as the next generation of
innovative intelligent instruments for environmental perception and
measurement. However, while satisfying the need for large-FoV photographic
imaging, panoramic imaging instruments are expected to have high resolution, no
blind area, miniaturization, and multi-dimensional intelligent perception, and
can be combined with artificial intelligence methods towards the next
generation of intelligent instruments, enabling deeper understanding and more
holistic perception of 360-degree real-world surrounding environments.
Fortunately, recent advances in freeform surfaces, thin-plate optics, and
metasurfaces provide innovative approaches to address human perception of the
environment, offering promising ideas beyond conventional optical imaging. In
this review, we begin with introducing the basic principles of panoramic
imaging systems, and then describe the architectures, features, and functions
of various panoramic imaging systems. Afterwards, we discuss in detail the
broad application prospects and great design potential of freeform surfaces,
thin-plate optics, and metasurfaces in panoramic imaging. We then provide a
detailed analysis on how these techniques can help enhance the performance of
panoramic imaging systems. We further offer a detailed analysis of applications
of panoramic imaging in scene understanding for autonomous driving and
robotics, spanning panoramic semantic image segmentation, panoramic depth
estimation, panoramic visual localization, and so on. Finally, we cast a
perspective on future potential and research directions for panoramic imaging
instruments."#29 pages, 14 figures, 348 references
computer science#computing research repository#computer vision and pattern recognition#HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance#Soshi Shimada, Vladislav Golyanik, Patrick Pérez, Weipeng Xu, Christian Theobalt#"Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics."#
computer science#computing research repository#computer vision and pattern recognition#RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation#Pingchuan Ma, Tao Du, Joshua B. Tenenbaum, Wojciech Matusik, Chuang Gan#"This work considers identifying parameters characterizing a physical system's
dynamic motion directly from a video whose rendering configurations are
inaccessible. Existing solutions require massive training data or lack
generalizability to unknown rendering configurations. We propose a novel
approach that marries domain randomization and differentiable rendering
gradients to address this problem. Our core idea is to train a
rendering-invariant state-prediction (RISP) network that transforms image
differences into state differences independent of rendering configurations,
e.g., lighting, shadows, or material reflectance. To train this predictor, we
formulate a new loss on rendering variances using gradients from differentiable
rendering. Moreover, we present an efficient, second-order method to compute
the gradients of this loss, allowing it to be integrated seamlessly into modern
deep learning frameworks. We evaluate our method in rigid-body and
deformable-body simulation environments using four tasks: state estimation,
system identification, imitation learning, and visuomotor control. We further
demonstrate the efficacy of our approach on a real-world example: inferring the
state and action sequences of a quadrotor from a video of its motion sequences.
Compared with existing methods, our approach achieves significantly lower
reconstruction errors and has better generalizability among unknown rendering
configurations."#ICLR Oral. Project page:
computer science#computing research repository#computer vision and pattern recognition#Invisible-to-Visible: Privacy-Aware Human Segmentation using Airborne Ultrasound via Collaborative Learning Probabilistic U-Net#Risako Tanigawa, Yasunori Ishii, Kazuki Kozuka, Takayoshi Yamashita#"Color images are easy to understand visually and can acquire a great deal of
information, such as color and texture. They are highly and widely used in
tasks such as segmentation. On the other hand, in indoor person segmentation,
it is necessary to collect person data considering privacy. We propose a new
task for human segmentation from invisible information, especially airborne
ultrasound. We first convert ultrasound waves to reflected ultrasound
directional images (ultrasound images) to perform segmentation from invisible
information. Although ultrasound images can roughly identify a person's
location, the detailed shape is ambiguous. To address this problem, we propose
a collaborative learning probabilistic U-Net that uses ultrasound and
segmentation images simultaneously during training, closing the probabilistic
distributions between ultrasound and segmentation images by comparing the
parameters of the latent spaces. In inference, only ultrasound images can be
used to obtain segmentation results. As a result of performance verification,
the proposed method could estimate human segmentations more accurately than
conventional probabilistic U-Net and other variational autoencoder models."#arXiv admin note: substantial text overlap with
computer science#computing research repository#computer vision and pattern recognition#Arbitrary Shape Text Detection via Boundary Transformer#Shi-Xue Zhang, Xiaobin Zhu, Chun Yang, Xu-Cheng Yin#"Arbitrary shape text detection is a challenging task due to its complexity
and variety, e.g, various scales, random rotations, and curve shapes. In this
paper, we propose an arbitrary shape text detector with a boundary transformer,
which can accurately and directly locate text boundaries without any
post-processing. Our method mainly consists of a boundary proposal module and
an iteratively optimized boundary transformer module. The boundary proposal
module consisting of multi-layer dilated convolutions will compute important
prior information (including classification map, distance field, and direction
field) for generating coarse boundary proposals meanwhile guiding the
optimization of boundary transformer. The boundary transformer module adopts an
encoder-decoder structure, in which the encoder is constructed by multi-layer
transformer blocks with residual connection while the decoder is a simple
multi-layer perceptron network (MLP). Under the guidance of prior information,
the boundary transformer module will gradually refine the coarse boundary
proposals via boundary deformation in an iterative manner. Furthermore, we
propose a novel boundary energy loss (BEL) which introduces an energy
minimization constraint and an energy monotonically decreasing constraint for
every boundary optimization step. Extensive experiments on publicly available
and challenging datasets demonstrate the state-of-the-art performance and
promising efficiency of our method."#13 pages, 12  is not the final version,just a preview. arXiv admin note: text overlap with
computer science#computing research repository#computer vision and pattern recognition#Deep Depth Completion: A Survey#Junjie Hu, Chenyu Bao, Mete Ozay, Chenyou Fan, Qing Gao, Honghai Liu, Tin Lun Lam#"Depth completion aims at predicting dense pixel-wise depth from a sparse map
captured from a depth sensor. It plays an essential role in various
applications such as autonomous driving, 3D reconstruction, augmented reality,
and robot navigation. Recent successes on the task have been demonstrated and
dominated by deep learning based solutions. In this article, for the first
time, we provide a comprehensive literature review that helps readers better
grasp the research trends and clearly understand the current advances. We
investigate the related studies from the design aspects of network
architectures, loss functions, benchmark datasets, and learning strategies with
a proposal of a novel taxonomy that categorizes existing methods. Besides, we
present a quantitative comparison of model performance on two widely used
benchmark datasets, including an indoor and an outdoor dataset. Finally, we
discuss the challenges of prior works and provide readers with some insights
for future research directions."#
computer science#computing research repository#computer vision and pattern recognition#AutoLC: Search Lightweight and Top-Performing Architecture for Remote Sensing Image Land-Cover Classification#Chenyu Zheng, Junjue Wang, Ailong Ma, Yanfei Zhong#"Land-cover classification has long been a hot and difficult challenge in
remote sensing community. With massive High-resolution Remote Sensing (HRS)
images available, manually and automatically designed Convolutional Neural
Networks (CNNs) have already shown their great latent capacity on HRS
land-cover classification in recent years. Especially, the former can achieve
better performance while the latter is able to generate lightweight
architecture. Unfortunately, they both have shortcomings. On the one hand,
because manual CNNs are almost proposed for natural image processing, it
becomes very redundant and inefficient to process HRS images. On the other
hand, nascent Neural Architecture Search (NAS) techniques for dense prediction
tasks are mainly based on encoder-decoder architecture, and just focus on the
automatic design of the encoder, which makes it still difficult to recover the
refined mapping when confronting complicated HRS scenes.
To overcome their defects and tackle the HRS land-cover classification
problems better, we propose AutoLC which combines the advantages of two
methods. First, we devise a hierarchical search space and gain the lightweight
encoder underlying gradient-based search strategy. Second, we meticulously
design a lightweight but top-performing decoder that is adaptive to the
searched encoder of itself. Finally, experimental results on the LoveDA
land-cover dataset demonstrate that our AutoLC method outperforms the
state-of-art manual and automatic methods with much less computational
consumption."#Early accepted by ICPR 2022
computer science#computing research repository#computer vision and pattern recognition#Recurrent Encoder-Decoder Networks for Vessel Trajectory Prediction with Uncertainty Estimation#Samuele Capobianco, Nicola Forti, Leonardo M. Millefiori, Paolo Braca, Peter Willett#"Recent deep learning methods for vessel trajectory prediction are able to
learn complex maritime patterns from historical Automatic Identification System
(AIS) data and accurately predict sequences of future vessel positions with a
prediction horizon of several hours. However, in maritime surveillance
applications, reliably quantifying the prediction uncertainty can be as
important as obtaining high accuracy. This paper extends deep learning
frameworks for trajectory prediction tasks by exploring how recurrent
encoder-decoder neural networks can be tasked not only to predict but also to
yield a corresponding prediction uncertainty via Bayesian modeling of epistemic
and aleatoric uncertainties. We compare the prediction performance of two
different models based on labeled or unlabeled input data to highlight how
uncertainty quantification and accuracy can be improved by using, if available,
additional information on the intention of the ship (e.g., its planned
destination)."#10 pages, 6 figures
computer science#computing research repository#computer vision and pattern recognition#An Objective Method for Pedestrian Occlusion Level Classification#Shane Gilroy, Martin Glavin, Edward Jones, Darragh Mullins#"Pedestrian detection is among the most safety-critical features of driver
assistance systems for autonomous vehicles. One of the most complex detection
challenges is that of partial occlusion, where a target object is only
partially available to the sensor due to obstruction by another foreground
object. A number of current pedestrian detection benchmarks provide annotation
for partial occlusion to assess algorithm performance in these scenarios,
however each benchmark varies greatly in their definition of the occurrence and
severity of occlusion. In addition, current occlusion level annotation methods
contain a high degree of subjectivity by the human annotator. This can lead to
inaccurate or inconsistent reporting of an algorithm's detection performance
for partially occluded pedestrians, depending on which benchmark is used. This
research presents a novel, objective method for pedestrian occlusion level
classification for ground truth annotation. Occlusion level classification is
achieved through the identification of visible pedestrian keypoints and through
the use of a novel, effective method of 2D body surface area estimation.
Experimental results demonstrate that the proposed method reflects the
pixel-wise occlusion level of pedestrians in images and is effective for all
forms of occlusion, including challenging edge cases such as self-occlusion,
truncation and inter-occluding pedestrians."#
computer science#computing research repository#computer vision and pattern recognition#Multi-Label Logo Recognition and Retrieval based on Weighted Fusion of Neural Features#Marisa Bernabeu, Antonio Javier Gallego, Antonio Pertusa#"Logo classification is a particular case of image classification, since these
may contain only text, images, or a combination of both. In this work, we
propose a system for the multi-label classification and similarity search of
logo images. The method allows obtaining the most similar logos on the basis of
their shape, color, business sector, semantics, general characteristics, or a
combination of such features established by the user. This is done by employing
a set of multi-label networks specialized in certain characteristics of logos.
The features extracted from these networks are combined to perform the
similarity search according to the search criteria established. Since the text
of logos is sometimes irrelevant for the classification, a preprocessing stage
is carried out to remove it, thus improving the overall performance. The
proposed approach is evaluated using the European Union Trademark (EUTM)
dataset, structured with the hierarchical Vienna classification system, which
includes a series of metadata with which to index trademarks. We also make a
comparison between well known logo topologies and Vienna in order to help
designers understand their correspondences. The experimentation carried out
attained reliable performance results, both quantitatively and qualitatively,
which outperformed the state-of-the-art results. In addition, since the
semantics and classification of brands can often be subjective, we also
surveyed graphic design students and professionals in order to assess the
reliability of the proposed method."#
computer science#computing research repository#computer vision and pattern recognition#RustSEG -- Automated segmentation of corrosion using deep learning#B. Burton, W.T. Nash, N. Birbilis#"The inspection of infrastructure for corrosion remains a task that is
typically performed manually by qualified engineers or inspectors. This task of
inspection is laborious, slow, and often requires complex access. Recently,
deep learning based algorithms have revealed promise and performance in the
automatic detection of corrosion. However, to date, research regarding the
segmentation of images for automated corrosion detection has been limited, due
to the lack of availability of per-pixel labelled data sets which are required
for model training. Herein, a novel deep learning approach (termed RustSEG) is
presented, that can accurately segment images for automated corrosion
detection, without the requirement of per-pixel labelled data sets for
training. The RustSEG method will first, using deep learning techniques,
determine if corrosion is present in an image (i.e. a classification task), and
then if corrosion is present, the model will examine what pixels in the
original image contributed to that classification decision. Finally, the method
can refine its predictions into a pixel-level segmentation mask. In ideal
cases, the method is able to generate precise masks of corrosion in images,
demonstrating that the automated segmentation of corrosion without per-pixel
training data is possible, addressing a significant hurdle in automated
infrastructure inspection."#
computer science#computing research repository#computer vision and pattern recognition#A Continual Deepfake Detection Benchmark: Dataset, Methods, and Essentials#Chuqiao Li, Zhiwu Huang, Danda Pani Paudel, Yabin Wang, Mohamad Shahbazi, Xiaopeng Hong, Luc Van Gool#"There have been emerging a number of benchmarks and techniques for the
detection of deepfakes. However, very few works study the detection of
incrementally appearing deepfakes in the real-world scenarios. To simulate the
wild scenes, this paper suggests a continual deepfake detection benchmark
(CDDB) over a new collection of deepfakes from both known and unknown
generative models. The suggested CDDB designs multiple evaluations on the
detection over easy, hard, and long sequence of deepfake tasks, with a set of
appropriate measures. In addition, we exploit multiple approaches to adapt
multiclass incremental learning methods, commonly used in the continual visual
recognition, to the continual deepfake detection problem. We evaluate several
methods, including the adapted ones, on the proposed CDDB. Within the proposed
benchmark, we explore some commonly known essentials of standard continual
learning. Our study provides new insights on these essentials in the context of
continual deepfake detection. The suggested CDDB is clearly more challenging
than the existing benchmarks, which thus offers a suitable evaluation avenue to
the future research. Our benchmark dataset and the source code will be made
publicly available."#
computer science#computing research repository#computer vision and pattern recognition#Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training#Jing Yang, Junwen Chen, Keiji Yanai#"In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings."#13 pages, 8 figures
computer science#computing research repository#computer vision and pattern recognition#Learning to Answer Visual Questions from Web Videos#Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid#"Recent methods for visual question answering rely on large-scale annotated
datasets. Manual annotation of questions and answers for videos, however, is
tedious, expensive and prevents scalability. In this work, we propose to avoid
manual annotation and generate a large-scale training dataset for video
question answering making use of automatic cross-modal supervision. We leverage
a question generation transformer trained on text data and use it to generate
question-answer pairs from transcribed video narrations. Given narrated videos,
we then automatically generate the HowToVQA69M dataset with 69M
video-question-answer triplets. To handle the open vocabulary of diverse
answers in this dataset, we propose a training procedure based on a contrastive
loss between a video-question multi-modal transformer and an answer
transformer. We introduce the zero-shot VideoQA task and the VideoQA feature
probe evaluation setting and show excellent results, in particular for rare
answers. Furthermore, our method achieves competitive results on MSRVTT-QA,
ActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA
dataset generation approach generalizes to another source of web video and text
data. We use our method to generate the WebVidVQA3M dataset from the WebVid
dataset, i.e., videos with alt-text annotations, and show its benefits for
training VideoQA models. Finally, for a detailed evaluation we introduce iVQA,
a new VideoQA dataset with reduced language bias and high-quality manual
annotations. Code, datasets and trained models are available at"#Accepted at the TPAMI Special Issue on the Best Papers of ICCV 2021. Journal extension of the conference paper . 16 pages, 13 figures
computer science#computing research repository#computer vision and pattern recognition#KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints#Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, Shunsuke Saito#"Image-based volumetric avatars using pixel-aligned features promise
generalization to unseen poses and identities. Prior work leverages global
spatial encodings and multi-view geometric consistency to reduce spatial
ambiguity. However, global encodings often suffer from overfitting to the
distribution of the training data, and it is difficult to learn multi-view
consistent reconstruction from sparse views. In this work, we investigate
common issues with existing spatial encodings and propose a simple yet highly
effective approach to modeling high-fidelity volumetric avatars from sparse
views. One of the key ideas is to encode relative spatial 3D information via
sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and
cross-dataset domain gap. Our approach outperforms state-of-the-art methods for
head reconstruction. On human body reconstruction for unseen subjects, we also
achieve performance comparable to prior work that uses a parametric human body
model and temporal feature aggregation. Our experiments show that a majority of
errors in prior work stem from an inappropriate choice of spatial encoding and
thus we suggest a new direction for high-fidelity image-based avatar modeling."#The project page is available at
computer science#computing research repository#computer vision and pattern recognition#Classification and mapping of low-statured 'shrubland' cover types in post-agricultural landscapes of the US Northeast#Michael J Mahoney, Lucas K Johnson, Colin M Beier#"Context: Novel plant communities reshape landscapes and pose challenges for
land cover classification and mapping that can constrain research and
stewardship efforts. In the US Northeast, emergence of low-statured woody
vegetation, or 'shrublands', instead of secondary forests in post-agricultural
landscapes is well-documented by field studies, but poorly understood from a
landscape perspective, which limits the ability to systematically study and
manage these lands. Objectives: To address gaps in classification/mapping of
low-statured cover types where they have been historically rare, we developed
models to predict 'shrubland' distributions at 30m resolution across New York
State (NYS), using machine learning and model ensembling techniques to
integrate remote sensing of structural (airborne LIDAR) and optical (satellite
imagery) properties of vegetation cover. We first classified a 1m canopy height
model (CHM), derived from a ""patchwork"" of available LIDAR coverages, to define
shrubland presence/absence. Next, these non-contiguous maps were used to train
a model ensemble based on temporally-segmented imagery to predict 'shrubland'
probability for the entire study landscape (NYS). Results: Approximately 2.5%
of the CHM coverage area was classified as shrubland. Models using Landsat
predictors trained on the classified CHM were effective at identifying
shrubland (test set AUC=0.893, real-world AUC=0.904), in discriminating between
shrub/young forest and other cover classes, and produced qualitatively sensible
maps, even when extending beyond the original training data. Conclusions: After
ground-truthing, we expect these shrubland maps and models will have many
research and stewardship applications including wildlife conservation, invasive
species mitigation and natural climate solutions."#29 pages (19 main text, 10 supplementary materials); 11 figures (10 main text, 1 supplementary materials), 10 tables (3 main text, 7 supplementary materials). Submitted to Landscape Ecology
computer science#computing research repository#computer vision and pattern recognition#Metric Learning based Interactive Modulation for Real-World Super-Resolution#Chong Mou, Yanze Wu, Xintao Wang, Chao Dong, Jian Zhang, Ying Shan#"Interactive image restoration aims to restore images by adjusting several
controlling coefficients, which determine the restoration strength. Existing
methods are restricted in learning the controllable functions under the
supervision of known degradation types and levels. They usually suffer from a
severe performance drop when the real degradation is different from their
assumptions. Such a limitation is due to the complexity of real-world
degradations, which can not provide explicit supervision to the interactive
modulation during training. However, how to realize the interactive modulation
in real-world super-resolution has not yet been studied. In this work, we
present a Metric Learning based Interactive Modulation for Real-World
Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised
degradation estimation strategy to estimate the degradation level in real-world
scenarios. Instead of using known degradation levels as explicit supervision to
the interactive mechanism, we propose a metric learning strategy to map the
unquantifiable degradation levels in real-world scenarios to a metric space,
which is trained in an unsupervised manner. Moreover, we introduce an anchor
point strategy in the metric learning process to normalize the distribution of
metric space. Extensive experiments demonstrate that the proposed MM-RealSR
achieves excellent modulation and restoration performance in real-world
super-resolution. Codes are available at
."#
statistics#statistics#methodology#Bayesian clustering of multiple zero-inflated outcomes#Beatrice Franzolini, Andrea Cremaschi, Willem van den Boom, Maria De Iorio#"Several applications involving counts present a large proportion of zeros
(excess-of-zeros data). A popular model for such data is the Hurdle model,
which explicitly models the probability of a zero count, while assuming a
sampling distribution on the positive integers. We consider data from multiple
count processes. In this context, it is of interest to study the patterns of
counts and cluster the subjects accordingly. We introduce a novel Bayesian
nonparametric approach to cluster multiple, possibly related, zero-inflated
processes. We propose a joint model for zero-inflated counts, specifying a
Hurdle model for each process with a shifted Negative Binomial sampling
distribution. Conditionally on the model parameters, the different processes
are assumed independent, leading to a substantial reduction in the number of
parameters as compared to traditional multivariate approaches. The
subject-specific probabilities of zero-inflation and the parameters of the
sampling distribution are flexibly modelled via an enriched finite mixture with
random number of components. This induces a two-level clustering of the
subjects based on the zero/non-zero patterns (outer clustering) and on the
sampling distribution (inner clustering). Posterior inference is performed
through tailored MCMC schemes. We demonstrate the proposed approach on an
application involving the use of the messaging service WhatsApp."#
statistics#statistics#methodology#Tuning Parameter Selection for Penalized Estimation via R2#Julia Holter, Jonathan Stallrich#"The tuning parameter selection strategy for penalized estimation is crucial
to identify a model that is both interpretable and predictive. However, popular
strategies (e.g., minimizing average squared prediction error via
cross-validation) tend to select models with more predictors than necessary.
This paper proposes a simple, yet powerful cross-validation strategy based on
maximizing squared correlations between the observed and predicted values,
rather than minimizing squared error loss. The strategy can be applied to all
penalized least-squares estimators and we show that, under certain conditions,
the metric implicitly performs a bias adjustment. Specific attention is given
to the lasso estimator, in which our strategy is closely related to the relaxed
lasso estimator. We demonstrate our approach on a functional variable selection
problem to identify optimal placement of surface electromyogram sensors to
control a robotic hand prosthesis."#
statistics#statistics#methodology#Principal Amalgamation Analysis for Microbiome Data#Yan Li, Gen Li, Kun Chen#"In recent years microbiome studies have become increasingly prevalent and
large-scale. Through high-throughput sequencing technologies and
well-established analytical pipelines, relative abundance data of operational
taxonomic units and their associated taxonomic structures are routinely
produced. Since such data can be extremely sparse and high dimensional, there
is often a genuine need for dimension reduction to facilitate data
visualization and downstream statistical analysis. We propose Principal
Amalgamation Analysis (PAA), a novel amalgamation-based and taxonomy-guided
dimension reduction paradigm for microbiome data. Our approach aims to
aggregate the compositions into a smaller number of principal compositions,
guided by the available taxonomic structure, by minimizing a properly measured
loss of information. The choice of the loss function is flexible and can be
based on familiar diversity indices for preserving either within-sample or
between-sample diversity in the data. To enable scalable computation, we
develop a hierarchical PAA algorithm to trace the entire trajectory of
successive simple amalgamations. Visualization tools including dendrogram,
scree plot, and ordination plot are developed. The effectiveness of PAA is
demonstrated using gut microbiome data from a preterm infant study and an HIV
infection study."#
statistics#statistics#methodology#Shared Frailty Methods for Complex Survival Data: A Review of Recent Advances#Malka Gorfine, David M. Zucker#"Dependent survival data arise in many contexts. One context is clustered
survival data, where survival data are collected on clusters such as families
or medical centers. Dependent survival data also arise when multiple survival
times are recorded for each individual. Frailty models is one common approach
to handle such data. In frailty models, the dependence is expressed in terms of
a random effect, called the frailty. Frailty models have been used with both
Cox proportional hazards model and the accelerated failure time model. This
paper reviews recent developments in the area of frailty models in a variety of
settings. In each setting we provide a detailed model description, assumptions,
available estimation methods, and R packages."#22 pages, 1 figure, 2 tables
statistics#statistics#methodology#Private Hypothesis Testing for Social Sciences#Ajinkya K Mulay, Sean Lane, Erin Hennes#"While running any experiment, we often have to consider the statistical power
to ensure an effective study. Statistical power or power ensures that we can
observe an effect with high probability if such a true effect exists. However,
several studies lack the appropriate planning for determining the optimal
sample size to ensure adequate power. Thus, careful planning ensures that the
power remains high even under high measurement errors while keeping the type 1
error constrained. We study the impact of differential privacy on experiments
and theoretically analyze the change in sample size required due to the
Gaussian mechanisms. Further, we provide an empirical method to improve the
accuracy of private statistics with simple bootstrapping."#Under review at Theory and Practice of Differential Privacy (TPDP) 2022
statistics#statistics#methodology#Leveraging baseline covariates to analyze small cluster-randomized trials with a rare binary outcome#Angela Y. Zhu, Nandita Mitra, Karla Hemming, Michael O. Harhay, Fan Li#"Cluster-randomized trials (CRTs) involve randomizing entire groups of
participants to treatment arms, but are often comprised of a limited number of
available clusters. While covariate adjustment can account for chance
imbalances between treatment arms and increase statistical efficiency in
individually-randomized trials, analytical methods for covariate adjustment in
small CRTs have received little attention to date. In this paper, we
systematically investigate, through extensive simulations, the operating
characteristics of propensity score weighting and multivariable regression as
two covariate adjustment strategies for estimating the participant-average
causal effect in small CRTs with a rare binary outcome and identify scenarios
where each covariate adjustment strategy has a relative efficiency advantage
over the other to make practical recommendations. Beyond efficiency
considerations, we also examined the finite-sample performance of the sandwich
variance estimators associated with propensity score weighting and
multivariable regression for quantifying the uncertainty in estimating the
participant-average treatment effect. We found that the \citet{mancl2001} type
bias-corrected sandwich variance estimator tends to provide the closest to
nominal coverage for both propensity score weighting and multivariable
regression estimators, extending the existing recommendations for unadjusted
analysis of CRTs to accommodate covariate adjustment in small CRTs with a rare
binary outcome. To illustrate the practical consequences of these various
adjustment approaches, we reanalyzed a recent CRT testing a sedation protocol
in $31$ pediatric intensive care units."#
statistics#statistics#methodology#Demystifying Inferential Models: A Fiducial Perspective#Yifan Cui, Jan Hannig#"Inferential models have recently gained in popularity for valid uncertainty
quantification. In this paper, we investigate inferential models by exploring
relationships between inferential models, fiducial inference, and confidence
curves. In short, we argue that from a certain point of view, inferential
models can be viewed as fiducial distribution based confidence curves. We show
that all probabilistic uncertainty quantification of inferential models is
based on a collection of sets we name principled sets and principled
assertions."#
statistics#statistics#methodology#Modeling panels of extremes#Debbie J. Dupuis, Sebastian Engelke, Luca Trapin#"Extreme value applications commonly employ regression techniques to capture
cross-sectional heterogeneity or time-variation in the data. Estimation of the
parameters of an extreme value regression model is notoriously challenging due
to the small number of observations that are usually available in applications.
When repeated extreme measurements are collected on the same individuals, i.e.,
a panel of extremes is available, pooling the observations in groups can
improve the statistical inference. We study three data sets related to risk
assessment in finance, climate science, and hydrology. In all three cases, the
problem can be formulated as an extreme value panel regression model with a
latent group structure and group-specific parameters. We propose a new
algorithm that jointly assigns the individuals to the latent groups and
estimates the parameters of the regression model inside each group. Our method
efficiently recovers the underlying group structure without prior information,
and for the three data sets it provides improved return level estimates and
helps answer important domain-specific questions."#
statistics#statistics#methodology#Data-Driven Optimal Sensor Placement for High-Dimensional System Using Annealing Machine#Tomoki Inoue, Tsubasa Ikami, Yasuhiro Egami, Hiroki Nagai, Yasuo Naganuma, Koichi Kimura, Yu Matsuda#"We propose a novel method for solving optimal sensor placement problem for
high-dimensional system using an annealing machine. The sensor points are
calculated as a maximum clique problem of the graph, the edge weight of which
is determined by the proper orthogonal decomposition (POD) mode obtained from
data based on the fact that a high-dimensional system usually has a
low-dimensional representation. Since the maximum clique problem is equivalent
to the independent set problem of the complement graph, the independent set
problem is solved using Fujitsu Digital Annealer. As a demonstration of the
proposed method, the pressure distribution induced by the Kármán vortex
street behind a square cylinder is reconstructed based on the pressure data at
the calculated sensor points. The pressure distribution is measured by
pressure-sensitive paint (PSP) technique, which is an optical flow diagnose
method. The root mean square errors (RMSEs) between the pressure measured by
pressure transducer and the reconstructed pressures (calculated from the
proposed method and an existing greedy method) at the same place are compared.
As the result, the similar RMSE is achieved by the proposed method using
approximately 1/5 number of sensor points obtained by the existing method. This
method is of great importance as a novel approach for optimal sensor placement
problem and a new engineering application of an annealing machine."#
statistics#statistics#methodology#Sequential Linear Discriminant Analysis in High Dimensions Using Individual Discriminant Functions#Seungchul Baek#"High dimensional classification has been highlighted for last two decades and
much research has been conducted in order to circumvent challenges encountered
in high dimensions. While existing methods have focused mainly on developing
classification rules assuming independence of covariates or using
regularization on the sample covariance matrix or the sample mean vector or
among others, we propose a novel approach that employs the ""discriminatory
power"" of each covariate, selects a set of important variables yielding the
lowest misclassification rate empirically, and constructs the optimal linear
classifier with selected variables. We carry out simulation studies and analyze
real data sets to illustrate the performance of our proposed classifier by
comparing it with existing classifiers."#22 pages, 3 figures
statistics#statistics#methodology#An Accelerated Failure Time Regression Model for Illness-Death Data: A Frailty Approach#Lea Kats, Malka Gorfine#"This work presents a new model and estimation procedure for the illness-death
survival data where the hazard functions follow accelerated failure time (AFT)
models. A shared frailty variate induces positive dependence among failure
times of a subject for handling the unobserved dependency between the
non-terminal and the terminal failure times given the observed covariates.
Semi-parametric maximum likelihood estimation procedure is developed via a
kernel smoothed-aided EM algorithm, and variances are estimated by weighted
bootstrap. The model is presented in the context of existing frailty-based
illness-death models, emphasizing the contribution of the current work. The
breast cancer data of the Rotterdam tumor bank are analyzed using the proposed
and existing illness-death models. The results are contrasted and evaluated
based on a new graphical goodness-of-fit procedure. Simulation results and data
analysis nicely demonstrate the practical utility of the shared frailty variate
with the AFT regression model under the illness-death framework."#
statistics#statistics#methodology#Inference of multivariate exponential Hawkesprocesses with inhibition and application toneuronal activity#Anna Bonnet, Miguel Martinez Herrera, Maxime Sangnier#"The Hawkes process is a multivariate past-dependent point process used to
model the relationship of event occurrences between different phenomena.
Although the Hawkes process was originally introduced to describe excitation
interactions, which means that one event increases the chances of another
occurring, there has been a growing interest in modeling the opposite effect,
known as inhibition. In this paper, we propose a maximum likelihood approach to
estimate the interaction functions of a multivariate Hawkes process that can
account for both exciting and inhibiting effects. To the best of our knowledge,
this is the first exact inference procedure designed for such a general setting
in the frequentist framework. Our method includes a thresholding step in order
to recover the support of interactions and therefore to infer the connectivity
graph. A benefit of our method is to provide an explicit computation of the
log-likelihood, which enables in addition to perform a goodness-of-fit test for
assessing the quality of estimations. We compare our method to classical
approaches, which were developed in the linear framework and are not
specifically designed for handling inhibiting effects. We show that the
proposed estimator performs better on synthetic data than alternative
approaches. We also illustrate the application of our procedure to a neuronal
activity dataset, which highlights the presence of both exciting and inhibiting
effects between neurons."#
statistics#statistics#methodology#The saturated pairwise interaction Gibbs point process as a joint species distribution model#Ian Flint, Nick Golding, Peter Vesk, Yan Wang, Aihua Xia#"In an effort to effectively model observed patterns in the spatial
configuration of individuals of multiple species in nature, we introduce the
saturated pairwise interaction Gibbs point process. Its main strength lies in
its ability to model both attraction and repulsion within and between species,
over different scales. As such, it is particularly well-suited to the study of
associations in complex ecosystems. Based on the existing literature, we
provide an easy to implement fitting procedure as well as a technique to make
inference for the model parameters. We also prove that under certain hypotheses
the point process is locally stable, which allows us to use the well-known
`coupling from the past' algorithm to draw samples from the model. Different
numerical experiments show the robustness of the model. We study three
different ecological datasets, demonstrating in each one that our model helps
disentangle competing ecological effects on species' distribution."#
statistics#statistics#methodology#A Comparative Tutorial of Bayesian Sequential Design and Reinforcement Learning#Mauricio Tec, Yunshan Duan, Peter Müller#"Reinforcement Learning (RL) is a computational approach to reward-driven
learning in sequential decision problems. It implements the discovery of
optimal actions by learning from an agent interacting with an environment
rather than from supervised data. We contrast and compare RL with traditional
sequential design, focusing on simulation-based Bayesian sequential design
(BSD). Recently, there has been an increasing interest in RL techniques for
healthcare applications. We introduce two related applications as motivating
examples. In both applications, the sequential nature of the decisions is
restricted to sequential stopping. Rather than a comprehensive survey, the
focus of the discussion is on solutions using standard tools for these two
relatively simple sequential stopping problems. Both problems are inspired by
adaptive clinical trial design. We use examples to explain the terminology and
mathematical background that underlie each framework and map one to the other.
The implementations and results illustrate the many similarities between RL and
BSD. The results motivate the discussion of the potential strengths and
limitations of each approach."#5 figures
statistics#statistics#methodology#Forecast combinations: an over 50-year review#Xiaoqian Wang, Rob J Hyndman, Feng Li, Yanfei Kang#"Forecast combinations have flourished remarkably in the forecasting community
and, in recent years, have become part of the mainstream of forecasting
research and activities. Combining multiple forecasts produced from the single
(target) series is now widely used to improve accuracy through the integration
of information gleaned from different sources, thereby mitigating the risk of
identifying a single ""best"" forecast. Combination schemes have evolved from
simple combination methods without estimation, to sophisticated methods
involving time-varying weights, nonlinear combinations, correlations among
components, and cross-learning. They include combining point forecasts, and
combining probabilistic forecasts. This paper provides an up-to-date review of
the extensive literature on forecast combinations, together with reference to
available open-source software implementations. We discuss the potential and
limitations of various methods and highlight how these ideas have developed
over time. Some important issues concerning the utility of forecast
combinations are also surveyed. Finally, we conclude with current research gaps
and potential insights for future research."#
statistics#statistics#methodology#On Exact Feature Screening in Ultrahigh-dimensional Binary Classification#Sarbojit Roy, Soham Sarkar, Subhajit Dutta, Anil K. Ghosh#"In this article, we propose a new model-free feature screening method based
on energy distances for ultrahigh-dimensional binary classification problems.
Unlike existing methods, the cut-off involved in our procedure is data
adaptive. With a high probability, the screened set retains only features after
discarding all the noise variables. The proposed screening method is then
extended to identify pairs of variables that are marginally undetectable, but
have differences in their joint distributions. Finally, we build a classifier
which maintains coherence between the proposed feature selection criteria and
discrimination method, and also establish its risk consistency. An extensive
numerical study with simulated data sets and real benchmark data sets show
clear and convincing advantages of our classifier over what currently exists in
the literature."#Paper: 26 pages, Supplementary: 31 pages
statistics#statistics#methodology#Enhanced Change-Point Detection in Functional Means#Shuhao Jiao, Ngai-Hang Chan, Chun-Yip Yau#"A new dimension reduction methodology for change-point detection in
functional means is developed in this paper. The major advantage and novelty of
the proposed method is its efficiency in selecting basis functions that capture
the change, or jump, of functional means, leading to higher detection power,
especially when the functions cannot be sufficiently explained by a small
number of basis functions or are contaminated by random noises. The throughly
developed theoretical results demonstrate that, even when the change shrinks to
zero, the proposed approach can still detect the change asymptotically almost
surely. The numerical simulation studies justify the superiority of the
proposed approach to the method based on functional principal components and
the fully functional approach without dimension reduction. An application to
annual humidity trajectories was also included to illustrate the practical
superiority of the developed approach."#
statistics#statistics#methodology#On a wider class of prior distributions for graphical models#Abhinav Natarajan, Willem van den Boom, Kristoforus Bryant Odang, Maria De Iorio#"Gaussian graphical models are useful tools for conditional independence
structure inference of multivariate random variables. Unfortunately, Bayesian
inference of latent graph structures is challenging due to exponential growth
of $\mathcal{G}_n$, the set of all graphs in $n$ vertices. One approach that
has been proposed to tackle this problem is to limit search to subsets of
$\mathcal{G}_n$. In this paper, we study subsets that are vector subspaces with
the cycle space $\mathcal{C}_n$ as main example. We propose a novel prior on
$\mathcal{C}_n$ based on linear combinations of cycle basis elements and
present its theoretical properties. Using this prior, we implemented a Markov
chain Monte Carlo algorithm and show that (i) posterior edge inclusion
estimates compared to the standard technique are comparable despite searching a
smaller graph space and (ii) the vector space perspective enables
straightforward MCMC algorithms."#31 pages, 6 figures
statistics#statistics#methodology#Efficient and flexible estimation of natural mediation effects under intermediate confounding and monotonicity constraints#Kara E. Rudolph, Ivan Diaz#"Natural direct and indirect effects are mediational estimands that decompose
the average treatment effect and describe how outcomes would be affected by
contrasting levels of a treatment through changes induced in mediator values
(in the case of the indirect effect) or not through induced changes in the
mediator values (in the case of the direct effect). Natural direct and indirect
effects are not generally point-identifiable in the presence of a
treatment-induced confounder, however they may still be identified if one is
willing to assume monotonicity between a treatment and the treatment-induced
confounder. We argue that this assumption may be reasonable in the relatively
common encouragement-design trial setting where intervention is randomized
treatment assignment and the treatment-induced confounder is whether or not
treatment was actually taken/adhered to. We develop efficiency theory for the
natural direct and indirect effects under this monotonicity assumption, and use
it to propose a nonparametric, multiply robust estimator. We demonstrate the
finite sample properties of this estimator using a simulation study, and apply
it to data from the Moving to Opportunity Study to estimate the natural direct
and indirect effects of being randomly assigned to receive a Section 8 housing
voucher -- the most common form of federal housing assistance -- on risk
developing any mood or externalizing disorder among adolescent boys, possibly
operating through various school and community characteristics."#
statistics#statistics#methodology#Bayesian Capture-Recapture Models that Facilitate Recursive Computing#Mevin B Hooten, Michael R Schwob, Devin S Johnson, Jacob S. Ivan#"Ecologists increasingly rely on Bayesian capture-recapture models to estimate
abundance of wildlife populations. Capture-recapture models account for
imperfect detectability in individual-level presence data. A variety of
approaches have been used to implement such models, including integrated
likelihood, parameter-expanded data augmentation, and combinations of those.
Recently proposed conditional specifications have improved the stability of
algorithms for fitting capture-recapture models. We arrive at similar
conditional specifications of capture-recapture models by considering recursive
implementation strategies that facilitate fitting models to large data sets.
Our approach enjoys the same computational stability but also allows us to fit
the desired model in stages and leverage parallel computing resources. Our
model specification includes a component for the capture history of detected
individuals and another component for the sample size which is random before
observed. We demonstrate this approach using three examples including
simulation and two data sets resulting from capture-recapture studies of
different species."#
statistics#statistics#methodology#Hypothesis Tests with Functional Data for Surface Quality Change Detection in Surface Finishing Processes#Shilan Jin, Rui Tuo, Akash Tiwari, Satish Bukkapatnam, Chantel Aracne-Ruddle, Ariel Lighty, Haley Hamza, Yu Ding#"This work is concerned with providing a principled decision process for
stopping or tool-changing in a surface finishing process. The decision process
is supposed to work for products of non-flat geometry. The solution is based on
conducting hypothesis testing on the bearing area curves from two consecutive
stages of a surface finishing process. In each stage, the bearing area curves,
which are in fact the nonparametric quantile curves representing the surface
roughness, are extracted from surface profile measurements at a number of
sampling locations on the surface of the products. The hypothesis test of these
curves informs the decision makers whether there is a change in surface quality
induced by the current finishing action. When such change is detected, the
current action is deemed effective and should thus continue, while when no
change is detected, the effectiveness of the current action is then called into
question, signaling possibly some change in the course of action. Application
of the hypothesis testing-based decision procedure to both spherical and flat
surfaces demonstrates the effectiveness and benefit of the proposed method and
confirms its geometry-agnostic nature."#33 pages, 12 figures
statistics#statistics#methodology#Towards better reconciling randomized controlled trial and observational study findings: Efficient algorithms for building representative matched samples with enhanced external validity#Bo Zhang#"Many recent efforts center on assessing the ability of real-world evidence
(RWE) generated from nonrandomized observational data to provide results that
are compatible with those from randomized controlled trials (RCTs). One
noticeable endeavor is the RCT DUPLICATE initiative (Franklin et al., 2020). To
better reconcile findings from observational and trial data, it is desirable to
eliminate differences between the RCT and corresponding observational study
populations. We outline an efficient, network-flow-based statistical matching
algorithm that designs well-matched pairs from observational data that mimic
the covariates' distribution of a target population, e.g., the RCT study
population or a population of scientific interest. We demonstrate the
usefulness of the method by revisiting the inconsistency regarding a
cardioprotective effect of the hormone replacement therapy (HRT) in the Women's
Health Initiative (WHI) clinical trial and corresponding observational study.
We found that the discrepancy between the trial and observational study
persisted in a design that adjusted for study populations' cardiovascular risk
profile, but seemed to disappear in a study design that further adjusted for
the HRT initiation age and previous estrogen-plus-progestin use. The proposed
method is integrated into the R package match2C."#
statistics#statistics#methodology#Addendum on the scoring of Gaussian directed acyclic graphical models#Jack Kuipers, Giusi Moffa, David Heckerman#"We provide a correction to the expression for scoring Gaussian directed
acyclic graphical models derived in Geiger and Heckerman [Ann. Statist. 30
(2002) 1414-1440] and discuss how to evaluate the score efficiently."#Published in at  the Annals of Statistics () by the Institute of Mathematical Statistics (). Typo in definition of R and one sentence corrected
statistics#statistics#methodology#Partition MCMC for inference on acyclic digraphs#Jack Kuipers, Giusi Moffa#"Acyclic digraphs are the underlying representation of Bayesian networks, a
widely used class of probabilistic graphical models. Learning the underlying
graph from data is a way of gaining insights about the structural properties of
a domain. Structure learning forms one of the inference challenges of
statistical graphical models.
MCMC methods, notably structure MCMC, to sample graphs from the posterior
distribution given the data are probably the only viable option for Bayesian
model averaging. Score modularity and restrictions on the number of parents of
each node allow the graphs to be grouped into larger collections, which can be
scored as a whole to improve the chain's convergence. Current examples of
algorithms taking advantage of grouping are the biased order MCMC, which acts
on the alternative space of permuted triangular matrices, and non ergodic edge
reversal moves.
Here we propose a novel algorithm, which employs the underlying combinatorial
structure of DAGs to define a new grouping. As a result convergence is improved
compared to structure MCMC, while still retaining the property of producing an
unbiased sample. Finally the method can be combined with edge reversal moves to
improve the sampler further."#Revised version. 34 pages, 16 figures. R code available at
statistics#statistics#methodology#Minimum Cost Intervention Design for Causal Effect Identification#Sina Akbari, Jalal Etesami, Negar Kiyavash#"Pearl's do calculus is a complete axiomatic approach to learn the
identifiable causal effects from observational data. When such an effect is not
identifiable, it is necessary to perform a collection of often costly
interventions in the system to learn the causal effect. In this work, we
consider the problem of designing the collection of interventions with the
minimum cost to identify the desired effect. First, we prove that this problem
is NP-hard, and subsequently propose an algorithm that can either find the
optimal solution or a logarithmic-factor approximation of it. This is done by
establishing a connection between our problem and the minimum hitting set
problem. Additionally, we propose several polynomial-time heuristic algorithms
to tackle the computational complexity of the problem. Although these
algorithms could potentially stumble on sub-optimal solutions, our simulations
show that they achieve small regrets on random graphs."#31 pages, 10 figures, ICML2022
statistics#statistics#methodology#Choosing Exogeneity Assumptions in Potential Outcome Models#Matthew A. Masten, Alexandre Poirier#"There are many kinds of exogeneity assumptions. How should researchers choose
among them? When exogeneity is imposed on an unobservable like a potential
outcome, we argue that the form of exogeneity should be chosen based on the
kind of selection on unobservables it allows. Consequently, researchers can
assess the plausibility of any exogeneity assumption by studying the
distributions of treatment given the unobservables that are consistent with
that assumption. We use this approach to study two common exogeneity
assumptions: quantile and mean independence. We show that both assumptions
require a kind of non-monotonic relationship between treatment and the
potential outcomes. We discuss how to assess the plausibility of this kind of
treatment selection. We also show how to define a new and weaker version of
quantile independence that allows for monotonic treatment selection. We then
show the implications of the choice of exogeneity assumption for
identification. We apply these results in an empirical illustration of the
effect of child soldiering on wages."#"This paper supersedes our previous paper titled ""Interpreting Quantile Independence"" ()"
